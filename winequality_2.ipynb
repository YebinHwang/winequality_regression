{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Build Linear Regression Class on weights"
      ],
      "metadata": {
        "id": "VuqtSEfPNlQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Weights** contain both the slope and intercept. It is easier to access each value if we can provide a direct interface instead of asking the user to subscript from the array. \n",
        "- Recall the property decorator that can be used to access the attribute of the object.\n",
        "- Will check the value of weights before running any operation to prevent calling the predict method before fitting the model."
      ],
      "metadata": {
        "id": "-Pa8LR1CNlTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(object):\n",
        "    \"\"\" Base regression model. Models the relationship between a scalar dependent variable y and the independent \n",
        "    variables X. \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "      # Initialize the weights\n",
        "      self.w = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      # Insert constant ones for bias weights\n",
        "      X = np.insert(X, 0, 1, axis=1)\n",
        "      self.w = np.linalg.pinv(np.dot(X.T, X)).dot(X.T).dot(y)\n",
        "\n",
        "    # define a \"coef\" getter\n",
        "    @property\n",
        "    def coef(self):\n",
        "      if self.w is None:\n",
        "        raise AttributeError('The coefficients do not exist')\n",
        "      return self.w[1:]\n",
        "\n",
        "    # define a \"intercept\" getter\n",
        "    @property\n",
        "    def intercept(self):\n",
        "      if self.w is None:\n",
        "          raise AttributeError('The intercept does not exist')\n",
        "      return self.w[0]\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "      # Check if the model has been fitted yet\n",
        "      if self.w is None:\n",
        "        raise AttributeError('You need to fit the model first before running the predictions')\n",
        "      # Insert constant ones for bias weights\n",
        "      X = np.insert(X, 0, 1, axis=1)\n",
        "      y_pred = X @ self.w\n",
        "\n",
        "      return y_pred"
      ],
      "metadata": {
        "id": "ksSLzG9bNT3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "id": "Q3ShVDQmOAA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.coef"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gEClH7FOBSI",
        "outputId": "f674d5c5-ccc1-4a5a-a31e-109a3bc43c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.49905523e-02, -1.08359026e+00, -1.82563948e-01,  1.63312696e-02,\n",
              "       -1.87422516e+00,  4.36133331e-03, -3.26457970e-03, -1.78811634e+01,\n",
              "       -4.13653146e-01,  9.16334412e-01,  2.76197700e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.intercept"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxlE0pxyOByg",
        "outputId": "77ea61a3-8421-4e93-cf0c-613d65240523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.96520806282637"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQVsWqqvOCb4",
        "outputId": "d6e33f98-8885-4641-f1a1-1fe81a3b1c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.03285045, 5.13787975, 5.20989474, ..., 5.94304255, 5.47075621,\n",
              "       6.00819633])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate model performance"
      ],
      "metadata": {
        "id": "xc-uTMw5OFjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "pL7Tn6zPODIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_regression.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lNnIVpLOIx1",
        "outputId": "c46fa59b-8b64-4d51-b74b-a6751c5e9251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l_regression.score(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHIClHTUOacC",
        "outputId": "69e8e88d-aa8b-4d9d-9923-5a95e3b0b90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36119824413213164"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l_regression.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L7rXLl_OdTK",
        "outputId": "6b9dd071-95e4-459e-cc67-41c2212a0d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3513885332505233"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The model shows a slightly higher score on the **training set** than the testing set"
      ],
      "metadata": {
        "id": "FOcRqpUzOe1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularized Regression: Ridge and Lasso"
      ],
      "metadata": {
        "id": "9pZr_gbqOn2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The root cause of the high model variance is due to the appearance of multicollinearity among the data features.\n",
        "- Intuitively the appearance of such hidden linear relationship makes our\n",
        "model over-confident about what it learns in the data --- which can **NOT** be generalized to unseen new test sets.\n",
        "- This suggests that as long as multicollinearity occurs, the linear\n",
        "coefficients estimated by the normal equation can be larger (in magnitude) than the un-observed true model’s coefficients."
      ],
      "metadata": {
        "id": "0ZCc0MykOv3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Gradient Descent**\n",
        "  - We first initialize the weights of the linear model as small values, say $\\beta^0$\n",
        "  - for `i` in `1:n_iterations`:\n",
        "    - Calculate the cost: $RSS(\\beta^i) = (y-X\\beta^i)^T(y-X\\beta^i)$\n",
        "    - Calculate the gradient: $\\nabla=\\frac{\\partial RSS}{\\partial\\beta^i} = -2X^T(y-X\\beta^i)$\n",
        "    - Update the value of $\\beta$ by multiplying the gradient with a learning rate: $\\beta^{i+1} = \\beta^i - \\eta\\nabla$"
      ],
      "metadata": {
        "id": "SvMb57IuPXhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Lasso Regression"
      ],
      "metadata": {
        "id": "t8ea_UIxPQ7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The difference between regularized regression and normal linear regression is the cost function.\n",
        "- Since the penality term is **NOT** a smooth function, there is no closed form solution to Lasso model's slope coefficients. We can use gradient to find the best weights\n",
        "\n",
        "$$Cost = (y−X\\beta)^T(y−X\\beta)+\\lambda|\\beta|$$\n",
        "\n",
        "- The $\\lambda$ here controls how hard do we want to penalize the weights. The higher the value is, the more shrinkage it will receive."
      ],
      "metadata": {
        "id": "JFGUz4MQPUwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class l1_regularization():\n",
        "  \"\"\" Regularization for Lasso Regression \"\"\"\n",
        "  def __init__(self, alpha):\n",
        "    self.alpha = alpha\n",
        "  \n",
        "  def __call__(self, w):\n",
        "    return self.alpha * np.linalg.norm(w, 1)\n",
        "\n",
        "  def grad(self, w):\n",
        "    return self.alpha * np.sign(w)"
      ],
      "metadata": {
        "id": "3rOSUvySOeHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Ridge Regression"
      ],
      "metadata": {
        "id": "4Ofz4ZbIPfZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can see that the only difference between ridge and lasso is the regularization term.\n",
        "  - $Cost = (y−X\\beta)^T(y−X\\beta)+\\lambda\\beta^T\\beta$\n",
        "  - Ridge uses L2 norm (squared) and Lasso uses L1 norm.\n",
        "- It is a good idea to create a general regression class with ridge and lasso to be the subclasses\n"
      ],
      "metadata": {
        "id": "35zXp5fLP1Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class l2_regularization():\n",
        "  \"\"\" Regularization for Ridge Regression \"\"\"\n",
        "  def __init__(self, alpha):\n",
        "    self.alpha = alpha\n",
        "  \n",
        "  def __call__(self, w):\n",
        "    return self.alpha * w.T @ w\n",
        "\n",
        "  def grad(self, w):\n",
        "    return self.alpha * w"
      ],
      "metadata": {
        "id": "aEmJyd2dPeAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the general regression class"
      ],
      "metadata": {
        "id": "4ZSEwdbMP28N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class Regression(object):\n",
        "    \"\"\" Base regression model. Models the relationship between a scalar dependent variable y and the independent \n",
        "    variables X.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_iterations: float\n",
        "        The number of training iterations the algorithm will tune the weights for.\n",
        "    learning_rate: float\n",
        "        The step length that will be used when updating the weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_iterations, learning_rate):\n",
        "      self.n_iterations = n_iterations\n",
        "      self.learning_rate = learning_rate\n",
        "\n",
        "    def initialize_weights(self, n_features):\n",
        "      \"\"\" Initialize weights randomly [-1/p, 1/p] \"\"\"\n",
        "      limit = 1/math.sqrt(n_features)\n",
        "      self.w = np.random.uniform(-limit, limit, (n_features, ))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      # Insert constant ones for bias weights\n",
        "      X = np.insert(X, 0, 1, axis=1)\n",
        "      # Initialize the weights\n",
        "      self.initialize_weights(n_features=X.shape[1])\n",
        "\n",
        "      # Perform gradient descent for n_iterations\n",
        "      for i in range(self.n_iterations):\n",
        "        y_pred = X.dot(self.w)\n",
        "        # Calculate the loss\n",
        "        mse = np.mean(0.5 * (y - y_pred)**2 + self.regularization(self.w))\n",
        "        # Calculate the gradient\n",
        "        # grad_w = (-(y - y_pred).dot(X)  + self.regularization.grad(self.w))/ X.shape[0]\n",
        "        grad_w = (-(y - y_pred) @ X  + self.regularization.grad(self.w))/ X.shape[0]\n",
        "        self.w -= self.learning_rate * grad_w\n",
        "\n",
        "\n",
        "    # define a \"coef\" getter\n",
        "    @property\n",
        "    def coef(self):\n",
        "      if self.w is None:\n",
        "        raise AttributeError('The coefficients do not exist')\n",
        "      return self.w[1:]\n",
        "\n",
        "    # define a \"intercept\" getter\n",
        "    @property\n",
        "    def intercept(self):\n",
        "      if self.w is None:\n",
        "          raise AttributeError('The intercept does not exist')\n",
        "      return self.w[0]\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "      # Check if the model has been fitted yet\n",
        "      if self.w is None:\n",
        "        raise AttributeError('You need to fit the model first before running the predictions')\n",
        "      # Insert constant ones for bias weights\n",
        "      X = np.insert(X, 0, 1, axis=1)\n",
        "      y_pred = X @ self.w\n",
        "\n",
        "      return y_pred"
      ],
      "metadata": {
        "id": "5s8E1g3oP19q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression Class"
      ],
      "metadata": {
        "id": "KqBsBc6LP5yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RidgeRegression(Regression):\n",
        "    \"\"\"Also referred to as Tikhonov regularization. Linear regression model with a regularization factor.\n",
        "    Model that tries to balance the fit of the model with respect to the training data and the complexity\n",
        "    of the model. A large regularization factor with decreases the variance of the model.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_iterations: float\n",
        "        The number of training iterations the algorithm will tune the weights for.\n",
        "    learning_rate: float\n",
        "        The step length that will be used when updating the weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, reg_factor, n_iterations=1000, learning_rate=0.001):\n",
        "      super().__init__(n_iterations, learning_rate)\n",
        "      # Adding the regularization term\n",
        "      self.regularization = l2_regularization(alpha=reg_factor)"
      ],
      "metadata": {
        "id": "SfvwXOlaP4Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso Regression Class"
      ],
      "metadata": {
        "id": "q_pBCOdeP66R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LassoRegression(Regression):\n",
        "    \"\"\"Linear regression model with a regularization factor which does both variable selection \n",
        "    and regularization. Model that tries to balance the fit of the model with respect to the training \n",
        "    data and the complexity of the model. A large regularization factor with decreases the variance of \n",
        "    the model and do para.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    reg_factor: float\n",
        "        The factor that will determine the amount of regularization and featureshrinkage. \n",
        "    n_iterations: float\n",
        "        The number of training iterations the algorithm will tune the weights for.\n",
        "    learning_rate: float\n",
        "        The step length that will be used when updating the weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, reg_factor, n_iterations=1000, learning_rate=0.001):\n",
        "      super().__init__(n_iterations, learning_rate)\n",
        "      # Adding the regularization term\n",
        "      self.regularization = l1_regularization(alpha=reg_factor)"
      ],
      "metadata": {
        "id": "1ml409sgP6vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need fit the scaler using only the training set but not the test set \n",
        "> to prevent data leakage, meaning that the mean and standard deviation is calcalted from the training set.\n",
        "\n",
        "will be using the **StandardScaler class** from the scikit-learn package."
      ],
      "metadata": {
        "id": "iz4a6GdNwcyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "tY5yaPOhP9Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso = LassoRegression(reg_factor = 1)\n",
        "lasso.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "aIxGvdGNwm3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso.coef"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv9zZ8_Pwm6X",
        "outputId": "a7166616-e4c5-4fdf-9d08-e3e9cf8d95f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.23913027, -0.07444481,  0.1355236 , -0.0750558 ,  0.09462986,\n",
              "        0.03374044,  0.01253292, -0.12177713,  0.20133235, -0.01269698,\n",
              "        0.31258495])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}